# -*- coding: utf-8 -*-
"""ClothModel2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YKXKIdftOttKeAETh2EBBcb9296ANcEf
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))
gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)

from google.colab import drive
drive.mount('/content/drive')

# print(net_dir)
# !drive/MyDrive/Research/network
# !ls

"""# Drive disconnect"""

# ##RUN THIS AT THE END TO GET THE NETWORK
# drive.flush_and_unmount()
# print('All changes made in this colab session should now be visible in Drive.')

"""# Models"""

# !pip3 install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-linux_x86_64.whl
# import torch
# import torch.nn as nn
!pip install torch==1.4.0 torchvision==0.5.0


import torch
import torch.nn as nn

import torch.nn.functional as F
eps_l2_norm = 1e-10
eps_sqrt = 1e-6
import torch.nn.functional as F

def weights_init_orthogonal(l):
    if isinstance(l, nn.Conv2d) or isinstance(l, nn.Conv1d):
        nn.init.orthogonal_(l.weight.data)
    return

def weights_init_zero(l):
    if isinstance(l, nn.Conv2d) or isinstance(l, nn.Conv1d):
        nn.init.constant_(l.weight.data, 0)
        nn.init.constant_(l.bias.data, 0)
        return

def desc_l2norm(desc):
    '''descriptors with shape NxC or NxCxHxW'''
    desc = desc / desc.pow(2).sum(dim=1, keepdim=True).add(eps_l2_norm).pow(0.5)
    return desc

def get_sparse_desc_grid(descr_FC, xy):
    _, _, h, w = descr_FC.size()
    xy[:, 0] = xy[:, 0].div(w - 1)
    xy[:, 1] = xy[:, 1].div(h - 1)
    xy = xy * 2 - 1
    xy.unsqueeze_(0).unsqueeze_(2)
    descrs = torch.nn.functional.grid_sample(descr_FC, xy, align_corners=False)
    descrs = descrs.squeeze().t()
    return descrs


def weights_init_zero(l):
    if isinstance(l, nn.Conv2d) or isinstance(l, nn.Conv1d):
        nn.init.constant_(l.weight.data, 0)
        nn.init.constant_(l.bias.data, 0)
        return


def weights_init_orthogonal(l):
    if isinstance(l, nn.Conv2d) or isinstance(l, nn.Conv1d):
        nn.init.orthogonal_(l.weight.data)
    return

class FRN(nn.Module):
    def __init__(self, num_features, eps=1e-6, is_bias=True, is_scale=True, is_eps_leanable=False):
        """
        weight = gamma, bias = beta

        beta, gamma:
            Variables of shape [1, 1, 1, C]. if TensorFlow
            Variables of shape [1, C, 1, 1]. if PyTorch
        eps: A scalar constant or learnable variable.
        """
        super(FRN, self).__init__()

        self.num_features = num_features
        self.init_eps = eps
        self.is_eps_leanable = is_eps_leanable
        self.is_bias = is_bias
        self.is_scale = is_scale


        self.weight = nn.parameter.Parameter(torch.Tensor(1, num_features, 1, 1), requires_grad=True)
        self.bias = nn.parameter.Parameter(torch.Tensor(1, num_features, 1, 1), requires_grad=True)
        if is_eps_leanable:
            self.eps = nn.parameter.Parameter(torch.Tensor(1), requires_grad=True)
        else:
            self.register_buffer('eps', torch.Tensor([eps]))
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.ones_(self.weight)
        nn.init.zeros_(self.bias)
        if self.is_eps_leanable:
            nn.init.constant_(self.eps, self.init_eps)

    def extra_repr(self):
        return 'num_features={num_features}, eps={init_eps}'.format(**self.__dict__)

    def forward(self, x):
        """
        0, 1, 2, 3 -> (B, H, W, C) in TensorFlow
        0, 1, 2, 3 -> (B, C, H, W) in PyTorch
        TensorFlow code
            nu2 = tf.reduce_mean(tf.square(x), axis=[1, 2], keepdims=True)
            x = x * tf.rsqrt(nu2 + tf.abs(eps))

            # This Code include TLU function max(y, tau)
            return tf.maximum(gamma * x + beta, tau)
        """
        # Compute the mean norm of activations per channel.
        nu2 = x.pow(2).mean(dim=[2, 3], keepdim=True)

        # Perform FRN.
        x = x * torch.rsqrt(nu2 + self.eps.abs())

        # Scale and Bias
        if self.is_scale:
            x = self.weight * x
        if self.is_bias:
            x = x + self.bias
        return x

class TLU(nn.Module):
    def __init__(self, num_features):
        """max(y, tau) = max(y - tau, 0) + tau = ReLU(y - tau) + tau"""
        super(TLU, self).__init__()
        self.num_features = num_features
        self.tau = nn.parameter.Parameter(torch.Tensor(1, num_features, 1, 1), requires_grad=True)
        self.reset_parameters()

    def reset_parameters(self):
        # nn.init.zeros_(self.tau)
        nn.init.constant_(self.tau, -1)

    def extra_repr(self):
        return 'num_features={num_features}'.format(**self.__dict__)

    def forward(self, x):
        return torch.max(x, self.tau)

class DepthNet(nn.Module):
    """
    """
    def __init__(self, is_bias=True, is_bias_FRN=True, is_xy=False, dim_desc=128):
        super(DepthNet, self).__init__()
        if is_xy:
            num_in_chan = 3
        else:
            num_in_chan = 1

        self.layer1 = nn.Sequential(
            FRN(num_in_chan, is_bias=is_bias_FRN),
            TLU(num_in_chan),
            nn.Conv2d(num_in_chan, 32, kernel_size=3, padding=1, bias=is_bias),
            FRN(32, is_bias=is_bias_FRN),
            TLU(32),
        )

        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=is_bias),
            FRN(32, is_bias=is_bias_FRN),
            TLU(32),
        )

        self.layer3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=is_bias),
            FRN(64, is_bias=is_bias_FRN),
            TLU(64),
        )

        self.layer4 = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=is_bias),
            FRN(64, is_bias=is_bias_FRN),
            TLU(64),
        )
        self.layer5 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=is_bias),
            FRN(128, is_bias=is_bias_FRN),
            TLU(128),
        )

        self.layer6 = nn.Sequential(
            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=is_bias),
            FRN(128, is_bias=is_bias_FRN),
            TLU(128),
        )

        self.layer_7x7 = nn.Sequential(
            nn.Dropout(0.2),
            nn.Conv2d(128, dim_desc, kernel_size=7, padding=3, bias=False),
        )

        # self.layer_5x9 = nn.Sequential(
        #     nn.Dropout(0.2),
        #     nn.Conv2d(128, self.dim_desc, kernel_size=(5, 9), padding=(2, 4), bias=False),
        # )
        #
        # self.layer_9x5 = nn.Sequential(
        #     nn.Dropout(0.2),
        #     nn.Conv2d(128, self.dim_desc, kernel_size=(9, 5), padding=(4, 2), bias=False),
        # )

        self.layer_BN_1D = nn.BatchNorm1d(dim_desc, affine=False)

        for layer in [self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6]:
            layer.apply(weights_init_orthogonal)#

        # for layer in [self.layer_7x7, self.layer_5x9, self.layer_9x5]:
        #     layer.apply(weights_init_orthogonal)#

        for layer in [self.layer_7x7]:
            layer.apply(weights_init_orthogonal)#

        return

    def forward(self, x, kpts=None, mode='eval'):
        '''
        :param x:
        :param kpts: should be in shape of Nx2
        :param mode:
        :return:
        '''

        for layer in [self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6]:
            x = layer(x)

        descrs = self.layer_7x7(x)
        descrs = get_sparse_desc_grid(descrs, kpts/4)
        descrs = self.layer_BN_1D(descrs)

        if mode == 'eval':
            descrs = desc_l2norm(descrs)

            return descrs

        elif mode == 'train':
            descrs_l2 = desc_l2norm(descrs)

            return descrs_l2, descrs




class DepthSegNet(nn.Module):
    """DescNet model definition
    """
    def __init__(self, is_bias=True, is_bias_FRN=True, is_xy=False, num_class=2):
        super(DepthSegNet, self).__init__()
        if is_xy:
            num_in_chan = 3
        else:
            num_in_chan = 1

        self.layer1 = nn.Sequential(
            FRN(num_in_chan, is_bias=is_bias_FRN),
            TLU(num_in_chan),
            nn.Conv2d(num_in_chan, 32, kernel_size=3, padding=1, bias=is_bias),
            FRN(32, is_bias=is_bias_FRN),
            TLU(32),
        )

        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=is_bias),
            FRN(32, is_bias=is_bias_FRN),
            TLU(32),
        )

        self.layer3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=is_bias),
            FRN(64, is_bias=is_bias_FRN),
            TLU(64),
        )

        self.layer4 = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=is_bias),
            FRN(64, is_bias=is_bias_FRN),
            TLU(64),
        )
        self.layer5 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=is_bias),
            FRN(128, is_bias=is_bias_FRN),
            TLU(128),
        )

        self.layer6 = nn.Sequential(
            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=is_bias),
            FRN(128, is_bias=is_bias_FRN),
            TLU(128),
        )

        self.layer_class = nn.Sequential(
            # nn.Dropout(0.2),
            nn.Conv2d(128, num_class, kernel_size=7, padding=3, bias=False),
        )

        self.layer_mask = nn.Sequential(
            # nn.Dropout(0.2),
            nn.Conv2d(128, 2, kernel_size=7, padding=3, bias=False),
        )


        for layer in [self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6]:
            layer.apply(weights_init_orthogonal)#

        for layer in [self.layer_class, self.layer_mask]:
            layer.apply(weights_init_orthogonal)#

        return

    def forward(self, x):
        '''
        :param x:
        :param kpts: should be in shape of Nx2
        :param mode:
        :return:
        '''
        h, w = x.size()[-2:]

        for layer in [self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6]:
            x = layer(x)

        mask = self.layer_mask(x)
        mask = F.interpolate(mask, size=(h, w), mode='bilinear', align_corners=True)

        seg = self.layer_class(x)
        seg = F.interpolate(seg, size=(h, w), mode='bilinear', align_corners=True)



        return mask, seg


class DepthSegNet_OneHead(nn.Module):
    """DescNet model definition
    """
    def __init__(self, is_bias=True, is_bias_FRN=True, is_xy=False, num_class=2):
        super(DepthSegNet_OneHead, self).__init__()
        if is_xy:
            num_in_chan = 3
        else:
            num_in_chan = 1

        self.layer1 = nn.Sequential(
            FRN(num_in_chan, is_bias=is_bias_FRN),
            TLU(num_in_chan),
            nn.Conv2d(num_in_chan, 32, kernel_size=3, padding=1, bias=is_bias),
            FRN(32, is_bias=is_bias_FRN),
            TLU(32),
        )

        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias=is_bias),
            FRN(32, is_bias=is_bias_FRN),
            TLU(32),
        )

        self.layer3 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=is_bias),
            FRN(64, is_bias=is_bias_FRN),
            TLU(64),
        )

        self.layer4 = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=is_bias),
            FRN(64, is_bias=is_bias_FRN),
            TLU(64),
        )
        self.layer5 = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=is_bias),
            FRN(128, is_bias=is_bias_FRN),
            TLU(128),
        )

        self.layer6 = nn.Sequential(
            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=is_bias),
            FRN(128, is_bias=is_bias_FRN),
            TLU(128),
        )

        self.layer_class = nn.Sequential(
            # nn.Dropout(0.2),
            nn.Conv2d(128, num_class, kernel_size=7, padding=3, bias=False),
        )
        
        self.out = nn.Sequential(
            # nn.Dropout(0.2),
            nn.Conv2d(512, num_class, kernel_size=7, padding=3, bias=False),
        )


        for layer in [self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6]:
            layer.apply(weights_init_orthogonal)#

        for layer in [self.layer_class]:
            layer.apply(weights_init_orthogonal)#

        return

    def forward(self, x):
        '''
        :param x:
        :param kpts: should be in shape of Nx2
        :param mode:
        :return:
        '''
        h, w = x.size()[-2:]

        for layer in [self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6]:
            x = layer(x)

        seg = self.layer_class(x)
        seg = F.interpolate(seg, size=(h, w), mode='bilinear', align_corners=True)
        
        return seg

"""# Unet"""

class UNET(nn.Module):
    """UNET model definition
    """
    def __init__(self, is_bias=True, is_bias_FRN=True, is_xy=False, num_class=2):
        super(UNET, self).__init__() ##Unet
        if is_xy:
            num_in_chan = 3
        else:
            num_in_chan = 1
      
        self.encoder1 = nn.Sequential(
            FRN(num_in_chan, is_bias=is_bias_FRN),
            TLU(num_in_chan),
            nn.Conv2d(num_in_chan, 32,  kernel_size=3, padding=1, bias=is_bias),
            FRN(32, is_bias=is_bias_FRN),
            TLU(32),
            nn.Conv2d(32,32,kernel_size=3, padding=1, bias=is_bias),
            FRN(32, is_bias=is_bias_FRN),
            TLU(32)
            
        )
        self.encoder2 = nn.Sequential(
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(32, 64,  kernel_size=3, padding=1, bias=is_bias),
            FRN(64, is_bias=is_bias_FRN),
            TLU(64),
            nn.Conv2d(64,64,kernel_size=3, padding=1, bias=is_bias),
            FRN(64, is_bias=is_bias_FRN),
            TLU(64)
          
        )
        self.encoder3 = nn.Sequential(
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(64, 128,  kernel_size=3, padding=1, bias=is_bias),
            FRN(128, is_bias=is_bias_FRN),
            TLU(128),
            nn.Conv2d(128,128,kernel_size=3, padding=1, bias=is_bias),
            FRN(128, is_bias=is_bias_FRN),
            TLU(128)
            
        )
        self.middle = nn.Sequential(
            nn.MaxPool2d(2, stride=2),
            nn.Conv2d(128, 256,  kernel_size=3, padding=1, bias=is_bias),
            FRN(256, is_bias=is_bias_FRN),
            TLU(256),
            nn.ConvTranspose2d(256,128,kernel_size=3, padding = 1,output_padding=1, stride = 2 , bias=is_bias),
            FRN(128, is_bias=is_bias_FRN),
            TLU(128)
            # nn.MaxPool2d(2, stride=2)
        )
        self.decoder3 = nn.Sequential(
            nn.Conv2d(256, 128,  kernel_size=3, padding=1, bias=is_bias),
            FRN(128, is_bias=is_bias_FRN),
            TLU(128),
            nn.ConvTranspose2d(128,64,kernel_size=3,padding = 1,output_padding=1, stride = 2,bias=is_bias),
            FRN(64, is_bias=is_bias_FRN),
            TLU(64)
            # nn.MaxPool2d(2, stride=2)
        )
        self.decoder2 = nn.Sequential(
            nn.Conv2d(128, 64,  kernel_size=3, padding=1, bias=is_bias),
            FRN(64, is_bias=is_bias_FRN),
            TLU(64),
            nn.ConvTranspose2d(64,32,kernel_size=3,padding = 1,output_padding=1, stride = 2, bias=is_bias),
            FRN(32, is_bias=is_bias_FRN),
            TLU(32)
        )
        self.decoder1 = nn.Sequential(
            nn.Conv2d(64, 32,  kernel_size=3, padding=1, bias=is_bias),
            FRN(32, is_bias=is_bias_FRN),
            TLU(32),
            nn.Conv2d(32,num_class,kernel_size=3,padding = 1, bias=is_bias),
            FRN(num_class, is_bias=is_bias_FRN),
            TLU(num_class)
        )

        self.layer_class = nn.Sequential(
            # nn.Dropout(0.2),
            nn.Conv2d(128, num_class, kernel_size=7, padding=3, bias=False)
        )
        # self.out = nn.Sequential(
        #     nn.Conv2d(32, num_in_chan,  kernel_size=3, padding=1, bias=is_bias),
        #     TLU(num_in_chan)
        
        # )


        for layer in [self.encoder1, self.encoder2, self.encoder3, self.middle, self.decoder3, self.decoder2, self.decoder1]:#,self.out
            layer.apply(weights_init_orthogonal)#

        for layer in [self.layer_class]:
            layer.apply(weights_init_orthogonal)#

        return

    def forward(self, x):
        '''
        :param x:
        :param kpts: should be in shape of Nx2
        :param mode:
        :return:
        '''
        h, w = x.size()[-2:]

        # for layer in [self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6]:
        #     x = layer(x)

        # seg = self.layer_class(x)
        # seg = F.interpolate(seg, size=(h, w), mode='bilinear', align_corners=True)
        # print("x ",x.size())
        xe1 = self.encoder1(x) #/2
        xe1 = F.leaky_relu(xe1)
        # print("xe1 ",xe1.size())
        xe2 = self.encoder2(xe1)
        xe2 = F.leaky_relu(xe2)
        # print("xe2 ",xe2.size())
        xe3 = self.encoder3(xe2)
        xe3 = F.leaky_relu(xe3)
        # print("xe3 ",xe3.size())
        mid = self.middle(xe3)
        mid = F.leaky_relu(mid)
        # print("mid ",mid.size())
        mid = torch.cat([mid, xe3],dim = 1)
        x = F.leaky_relu(x)
        xd3 = self.decoder3(mid)
        xd3 = F.leaky_relu(xd3)
        # print("xd3 ",xd3.size())
        xd3 = torch.cat([xd3, xe2],dim = 1)
        xd2 = self.decoder2(xd3)
        xd2 = F.leaky_relu(xd2)

        # print("xd2 ",xd2.size())
        xd2 = torch.cat([xd2, xe1],dim = 1)
        seg = self.decoder1(xd2)
        seg = F.relu(seg)
        # print("seg ",seg.size())

        # seg = self.out(xd1)
        # print("seg ",seg.size())

       




        return seg

# https://cv.iri.upc-csic.es/Dataset/ipalm_cloth4.tar.gz

import os
from pathlib import Path
from torch.utils.data import Dataset
import numpy as np
import cv2
# import OpenEXR
# import array
# import Imath
# FLOAT = Imath.PixelType(Imath.PixelType.FLOAT)
import matplotlib.pyplot as plt
import scipy.io
import torch
# from model import *
import torch.optim as optim
import argparse
import ast

"""# Dataloader"""

class IpalmDataset(Dataset):
    # num_sims: int = 80
    # num_sims: int = 10
    # num_cameras: int = 8
    # num_illums: int = 10
    num_sims: int = 1
    num_cameras: int = 36
    num_illums: int = 1
    cam_steps: int = 6

    def __init__(self, root_dir: str, mode: str = 'train', segmentation_mode: int = 1):
        self._root_dir: str = root_dir
        self._mode: str = mode
        self._segmentation_mode: int = segmentation_mode

        self._data_index = self.__create_data_index(root_dir)
        self._code_classes = self.__load_labels_file(self._segmentation_mode)

    def __load_labels_file(self, segmentation_mode: int) -> np.ndarray:
        segmentation_file_id: int = segmentation_mode + 1
        file_name: str = str((
                                     Path(self._root_dir) / f"labels_seg{segmentation_file_id}.txt"
                             ).resolve())

        with open(file_name) as f:
            lines_list = f.readlines()

        out = []
        for line in lines_list:
            # line coming in format "id [int, int, int]"
            line_list = line.split()
            # take array number and get rid of square brackets
            line_new = [line_list[1], line_list[2], line_list[3]]
            out.append(list(map(int, line_new)))
        return np.array(out)

    def __create_sims_ids(self):
        seed = 1234
        np.random.seed(seed)
        ids = np.arange(0,60)
        np.random.shuffle(ids)
        if self._mode == 'train':
            ids = ids[:50]
        elif self._mode == 'valid':
            ids = ids[50:]# 11 16
        else:
            raise NotImplementedError(f"Invalid mode: {self._mode}.")
        return ids

    def __create_data_index(self, root_dir: str):
        root_path: Path = Path(root_dir).resolve()
        sample_index_out = []
        sims_ids = self.__create_sims_ids()
        for sim_id in sims_ids:
            # print("Mode = ", self._mode)
            for cam_id_src in range(self.num_cameras):
                for cam_step in range(self.cam_steps):
                    cam_id_dst: int = (cam_id_src + cam_step) % self.num_cameras
                    for illum_id in range(self.num_illums):
                        sample_path_src = Path(f"sim{sim_id:04d}/Camera{cam_id_src:02d}")
                        sample_path_dst = Path(f"sim{sim_id:04d}/Camera{cam_id_dst:02d}")
                        if sample_path_src == sample_path_dst:
                            continue
                        sample_index = {}
                        sample_index["rgb"] = (
                            str(root_path / "rgb" / sample_path_src / f"0250_{illum_id:02d}.jpg")
                        )
                        sample_index["depth_src"] = (
                            str(root_path / "depth" / sample_path_src / "0250.exr")
                        )
                        sample_index["depth_dst"] = (
                            str(root_path / "depth" / sample_path_dst / "0250.exr")
                            
                        )
                        # print(str(root_path / "depth" / sample_path_dst / "0250.exr"))
                        ''''sample_index["norms_dst"] = (
                            str(root_path / "normals" / sample_path_dst / "Image0250.exr") 
                        )'''
                        sample_index["cam_src"] = (
                            # str(root_path / f"cameras/Camera{cam_id_src:02d}.txt") # v2
                            str(root_path / f"cameras/sim{sim_id:04d}/Camera{cam_id_src:02d}.txt")  # v3
                        )
                        sample_index["cam_dst"] = (
                            # str(root_path / f"cameras/Camera{cam_id_dst:02d}.txt") # v2
                            str(root_path / f"cameras/sim{sim_id:04d}/Camera{cam_id_dst:02d}.txt")  # v3
                        )
                        #######change here and the above
                        sample_index["seg_src"] = (
                            str(root_path / "segmentation" / sample_path_src / "01_0250.png")
                        )
                        sample_index["seg_dst"] = (
                            str(root_path / "segmentation" / sample_path_dst / "01_0250.png")
                        )


                        '''sample_index["segmentation"] = (
                            str(root_path / "segmentation" / sample_path / f"{self._segmentation_mode:02d}_0250.png")
                        )
                        sample_index["uvpixels"] = (
                            str(root_path / "uvpixels" / sample_path / "0250.npz")
                        )
                        sample_index["occlusions"] = (
                            str(root_path / "occlusions" / sample_path / "0250.npz")
                        )'''
                        sample_index_out.append(sample_index)

        
        # random.shuffle(sample_index_out)
        
        return sample_index_out

    '''def _image_to_labels(self, image):
       assert len(image.shape) == 3, image.shape
       H, W, C = image.shape
       labels = np.zeros((H, W), dtype=np.int64)
       for label_id, color in enumerate(self._code_classes):
           mask = (image == color).all(-1)
           labels[mask] = label_id
       return labels

    def _labels_to_image(self, labels):
       assert len(labels.shape) == 3, labels.shape
       B, H, W = labels.shape
       image = torch.zeros(B, 3, H, W, dtype=torch.uint8, device=labels.device)
       for label_id, label_color in enumerate(self._code_classes):
           mask = (labels == label_id).to(labels.device)
           for i in range(3):
              image[:, i].masked_fill_(mask, label_color[i])
       return image

    def _load_image_exr(self, file_path: str):
        data = OpenEXR.InputFile(file_path)
        data_list = [
           array.array('f', data.channel(CH, FLOAT)).tolist() for CH in ("R", "G", "B")
        ]
        data_array = np.transpose(np.reshape(data_list, (3, 512, 512)), (1, 2, 0))
        return (255. * data_array).astype(np.uint8)

    def _load_image_np(self, file_path: str):
        data = np.load(file_path)
        data_uv = data['arr_0']  # HxWx2
        return data_uv.astype(np.float32)

    def _predictions_to_image(self, labels):
       labels_max = torch.argmax(labels, dim=1)
       return self._labels_to_image(labels_max)

    def _map_uv_coords(self, input, uv_coords, occlusions=None):
        x = input
        grid = uv_coords
        occ = occlusions
        if len(x.shape) == 3:
            x = input[None] # 1xCxHxW
            grid = grid[None]
            occ = occ[None]
        grid = grid.permute(0, 2, 3, 1)  # BxHxWx2
        grid_norm = grid * 2 - 1  # in range -1/1
        x_out = torch.nn.functional.grid_sample(x, grid_norm)
        if occlusions is not None:
            x_occ = torch.nn.functional.grid_sample(occ, grid_norm)
            x_occ[x_occ==2.] = 0.
            x_out *= x_occ

        return x_out'''

    def _load_depth(self, file_path: str):
        depth = cv2.imread(file_path, cv2.IMREAD_ANYDEPTH)  # HxW
        depth = depth[None, :, :]  # 1xHxW
        depth[depth == np.inf] = 100
        return depth.astype(np.float32)

    def _load_seg(self, file_path: str):

        # def image_to_labels(image):
        #     assert len(image.shape) == 3, image.shape
        #     H, W, C = image.shape
        #     labels = np.zeros((H, W), dtype=np.int64)
        #     for label_id, color in enumerate(self._code_classes):
        #         mask = (image == color).all(-1)
        #         labels[mask] = label_id
        #     return labels

        def __image_to_labels(image):
            assert len(image.shape) == 3, image.shape
            H, W, C = image.shape
            labels = np.zeros((H, W), dtype=np.int64)
            for label_id, color in enumerate(self._code_classes):
                mask = (image == color ).all(-1)
                labels[mask] = label_id
            return labels


        image = cv2.imread(file_path)  # HxW
        label = __image_to_labels(image)

        return label.astype(np.float32)




    def _load_normals(self, file_path: str):
        normals = cv2.imread(file_path, cv2.IMREAD_ANYCOLOR | cv2.IMREAD_ANYDEPTH)  # HxWx3
        # normals = (normals + 1) / 2
        return normals.astype(np.float32).transpose((2, 0, 1))

    def _load_camera_file(self, file_path: str):
        with open(file_path) as f:
            content = f.readlines()
        content = [x.strip().split() for x in content[6:9]]
        proj_mat = np.array(content).astype(np.float32)
        # add row
        proj_mat = np.vstack([proj_mat, np.zeros(4)])
        proj_mat[..., -1, -1] = 1.
        return proj_mat

    def _load_camera_intrinsic(self, file_path: str):
        with open(file_path) as f:
            content = f.readlines()
        content = [x.strip().split() for x in content[1:4]]
        K = np.array(content).astype(np.float32)

        return K


    def __len__(self):
        return len(self._data_index)

    def __getitem__(self, idx):
        sample_paths = self._data_index[idx]
        # print(sample_paths)
        # load rgb
        image = cv2.imread(sample_paths['rgb'], cv2.IMREAD_COLOR)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # image = K.image_to_tensor(image).float() / 255.

        # load labels
        # label = cv2.imread(sample_paths['segmentation'], cv2.IMREAD_COLOR)
        # label = cv2.cvtColor(label, cv2.COLOR_BGR2RGB)
        # label = self._image_to_labels(label)

        # load depth
        depth_src = self._load_depth(sample_paths['depth_src'])
        depth_dst = self._load_depth(sample_paths['depth_dst'])

        seg_src = self._load_seg(sample_paths['seg_src'])
        seg_dst = self._load_seg(sample_paths['seg_dst'])


        # norms_dst = self._load_normals(sample_paths['norms_dst'])


        # load cameras and compute relative pose

        #commented
        # cam_src = self._load_camera_file(sample_paths['cam_src'])
        # cam_dst = self._load_camera_file(sample_paths['cam_dst'])

        # k_src = self._load_camera_intrinsic(sample_paths['cam_src'])
        # k_dst = self._load_camera_intrinsic(sample_paths['cam_dst'])

        # trans = (np.linalg.inv(cam_src) @ cam_dst).astype(np.float32)
        # commented 

        # load uvmap
        # uvpixels = self._load_image_np(sample_paths['uvpixels'])
        # uvpixels = K.image_to_tensor(uvpixels)

        # load occlusions
        # occlusions = self._load_image_np(sample_paths['occlusions'])
        # occlusions = K.image_to_tensor(occlusions)

        # map rgb
        # image_map = self._map_uv_coords(image, uvpixels, occlusions)[0]

        # return dict(image=image, depth=depth), dict(labels=label, uvpixels=uvpixels, rgb_map=image_map)
        return dict(image=image, depth_src=depth_src, seg_src=seg_src), \
               dict(depth_dst=depth_dst, seg_dst=seg_dst)
                #, k_src=k_src, trans=trans
                # norms_dst=norms_dst,
##_________________________________________-
#ALL NORM_DST CODE is commented out and CAMERA CODE TOO

import sys
sys.argv=['']
del sys
#fix found in "https://stackoverflow.com/questions/42249982/systemexit-2-error-when-calling-parse-args/42250234"

SMOOTH = 1e-6


def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):
    # You can comment out this line if you are passing tensors of equal shape
    # But if you are passing output from UNet or something it will most probably
    # be with the BATCH x 1 x H x W shape
    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W

    intersection = (outputs * labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0
    union = (outputs + labels).float().sum((1, 2))  # Will be zzero if both are 0

    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0

    # thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds

    return iou  # Or thresholded.mean() if you are interested in average across the batch


parser = argparse.ArgumentParser(description='pyTorch hynet_dense')
parser.add_argument('--network_root', type=str, default='drive/MyDrive/Research')  #  /home_nfs/yurun/mydata /home/yurun/Research/mydata


parser.add_argument('--epoch_max', type=int, default=50)
parser.add_argument('--num_iter_per_epoch', type=int, default=int(1000))


parser.add_argument('--is_CrossAtt', type=ast.literal_eval, default=False)
parser.add_argument('--is_SelfAtt', type=ast.literal_eval, default=False)
parser.add_argument('--is_xy', type=ast.literal_eval, default=False)


parser.add_argument('--optim_method', type=str, default='Adam')
parser.add_argument('--lr_scheduler', type=str, default='None')#CosineAnnealing
parser.add_argument('--lr', type=float, default=1e-2)
parser.add_argument('--drop_rate', type=float, default=0.2)

parser.add_argument('--suffix', type=str, default='')#
args = parser.parse_args()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

folder_name = 'IPALM'

if args.is_CrossAtt:
    folder_name += '_CrossAtt'
if args.is_SelfAtt:
    folder_name += '_SelfAtt'
if not args.is_SelfAtt and not args.is_SelfAtt:
    folder_name += '_Plain'

# folder_name += '_OneHead' ## NOT ONEHEAD NOW
#-----------------------------------------------------------------------------------------------------------------------------
folder_name += '_Unet_old_data'

is_merge_label = True
if is_merge_label:
    folder_name += '_MergeLable'

if len(args.suffix) > 0:
    folder_name += '-' + args.suffix

net_dir = os.path.join(args.network_root, 'network', folder_name)
print(net_dir)
if not os.path.exists(net_dir):
    os.makedirs(net_dir)
else:
    print('path already exists')

"""# Network load"""

# import pdb; pdb.set_trace()
# !pip install -Uqq ipdb
# import ipdb

# %debug
# net_train = DepthSegNet_OneHead(is_xy=args.is_xy, num_class=3)
# UNET
# net_train = DepthSegNet(is_xy=args.is_xy, num_class=3)

from torchsummary import summary

# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')



net_train = UNET(is_xy=args.is_xy, num_class=3)
net_train.cuda()

print(summary(net_train, input_size=(1, 512, 512)))
net_train.load_state_dict(torch.load('drive/MyDrive/Research/network/IPALM_Plain_Unet_old_data_MergeLable/net-epoch-7.pth'))


optimizer = optim.Adam(filter(lambda p: p.requires_grad, net_train.parameters()), lr=args.lr)

# root_dir: str = 'drive/MyDrive/Research/ipalm_cloth4'
root_dir: str = 'drive/MyDrive/Research/ipalm_cloth3'
dataset_train = IpalmDataset(root_dir, mode='train')
# ipdb.set_trace(context=6)
dataset_val = IpalmDataset(root_dir, mode='valid')
# ipdb.set_trace(context=6)

train_loader = torch.utils.data.DataLoader(dataset_train,shuffle= True)#,shuffle= True
val_loader = torch.utils.data.DataLoader(dataset_val,shuffle= True)#,shuffle= True

# detector = cv2.BRISK_create()#thresh=0
# H, W = 512, 512
# X = torch.FloatTensor(range(W)).unsqueeze(0).repeat(H, 1).unsqueeze(0)
# Y = torch.FloatTensor(range(H)).unsqueeze(0).repeat(W, 1).unsqueeze(0)

criterion_seg = nn.CrossEntropyLoss(reduction='none')
criterion_mask = nn.CrossEntropyLoss()

criterion = nn.CrossEntropyLoss(reduction='none')

softmax = nn.Softmax(dim=1)
num_iter = 100

"""#Train"""

import matplotlib.pyplot as plt

def train():
    net_train.train()
    running_loss = 0
    batch_loop = 0
    num_iter_per_epoch = len(dataset_train)
    for data, target in dataset_train:
        if batch_loop == num_iter - 1:
            break

        depth_src = data['depth_src']  # Bx1xHxW
        # k_src = data['k_src']
        # input_1 = torch.cat((X, Y, depth_src), dim=1)
        seg_src = data['seg_src']
        # print(seg_src)

        # trans = data['trans']  # Bx4x4
        depth_dst = target['depth_dst']  # Bx1xHxW
        # k_dst = target['k_dst']  # Bx1xHxW
        seg_dst = target['seg_dst']

        depth_src = torch.from_numpy(depth_src).to(device)
        depth_dst = torch.from_numpy(depth_dst).to(device)
        seg_src = torch.from_numpy(seg_src).to(device)
        seg_dst = torch.from_numpy(seg_dst).to(device)

        mask_src_depth = depth_src.lt(100).float().squeeze()
        mask_dst_depth = depth_dst.lt(100).float().squeeze()

        # mask_flaw_src = mask_src_depth.gt(0).squeeze() * seg_src.eq(0)
        # seg_src[mask_flaw_src] = 1
        #
        # mask_flaw_dst = mask_dst_depth.gt(0).squeeze() * seg_dst.eq(0)
        # seg_dst[mask_flaw_dst] = 1


        mask_src = seg_src.gt(0).float()
        mask_dst = seg_dst.gt(0).float()

        depth_src = depth_src * mask_src
        depth_dst = depth_dst * mask_dst

        if is_merge_label:
            ## for 01_250
            seg_src[seg_src.gt(1)] = 2
            seg_dst[seg_dst.gt(1)] = 2

        seg_src = seg_src.unsqueeze(0).long()
        seg_dst = seg_dst.unsqueeze(0).long()


        IDs = seg_src.unique()
        # print("Unique colours: ", IDs)

        if len(depth_src.size()) == 3:
            depth_src = depth_src.unsqueeze(0)
            depth_dst = depth_dst.unsqueeze(0)


        out_seg_src = net_train(depth_src)
        out_seg_dst = net_train(depth_dst)

        loss_seg_src = criterion(out_seg_src, seg_src).squeeze()
        loss_seg_dst = criterion(out_seg_dst, seg_dst).squeeze()

        loss = torch.Tensor([0]).to(device)

        # fig, axes = plt.subplots(2, 2, figsize = (30,10))
        

        
        seg_src = seg_src.squeeze()
        seg_dst = seg_dst.squeeze()

        for id in IDs:
            loss = loss + loss_seg_src[seg_src.eq(id)].mean() + loss_seg_dst[seg_dst.eq(id)].mean()
        f = plt.figure(figsize=(30,10))
        
        # ax1 = f.add_subplot(121)
        # ax2 = f.add_subplot(122)
        ax3 = f.add_subplot(121)
        ax4 = f.add_subplot(122)


        #
        # out_mask_src = softmax(out_mask_src).squeeze()[1]
        # ax1.plt(out_mask_src)
        # out_mask_dst = softmax(out_mask_dst).squeeze()[1]
        # ax2.plt(out_mask_dst)
        #
       
        # seg_src = seg_src.unsqueeze(0)
        # seg_dst = seg_dst.unsqueeze(0)

        # seg_src = softmax(seg_src) 
        # seg_src  = seg_src.cpu().detach().argmax(dim = 0).numpy()
        # axes[0,0].set_title('seg src train epoch: %i' %epoch_loop)
        # axes[0,0].imshow( seg_src)
        # axes[0,0].axis('off')

        # seg_dst = seg_dst.cpu().detach().argmax(dim = 0).numpy()
        # axes[1,0].set_title('seg dst train epoch: %i'%epoch_loop)
        # axes[1,0].imshow(seg_dst)
        # axes[1,0].axis('off')


        out_seg_src = softmax(out_seg_src).squeeze()#
        out_seg_src = out_seg_src.cpu().detach().argmax(dim = 0).numpy()
        ax3.set_title('out seg src train epoch: %i' %epoch_loop)
        ax3.imshow( out_seg_src)
        ax3.axis('off')

        # seg_dst = softmax(seg_dst)##
        

        out_seg_dst = softmax(out_seg_dst).squeeze()##
        out_seg_dst = out_seg_dst.cpu().detach().argmax(dim = 0).numpy()
        ax4.set_title('out seg dst train epoch: %i'%epoch_loop)
        ax4.imshow(out_seg_dst)
        ax4.axis('off')
        #
        # scipy.io.savemat('mask.mat', dict(mask_src=mask_src.detach().squeeze().cpu().numpy(), out_mask_src=out_mask_src.detach().squeeze().cpu().numpy()))
        # out_seg_src = out_seg_src.argmax(dim=0) * out_mask_src.gt(0.5)
        # scipy.io.savemat('seg.mat', dict(seg_src=seg_src.detach().squeeze().cpu().numpy(), out_seg_src=out_seg_src.detach().squeeze().cpu().numpy()))



        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        running_loss = running_loss + loss.item()
        print('epoch {}: {}/{}: loss: {:.4f}'.format(
            epoch_loop + 1,
            batch_loop + 1,
            num_iter_per_epoch,
            running_loss / (batch_loop + 1)))
        # print('epoch {}: {}/{}'.format(
        #     epoch_loop + 1,
        #     batch_loop + 1,
        #     num_iter_per_epoch,
        # ))
        batch_loop += 1

        if batch_loop == num_iter - 1:
            loss_train.append(running_loss/ (batch_loop + 1))


        # out_seg_src = softmax(out_seg_src).squeeze()  # * out_mask_src.gt(0.5).float().unsqueeze(0)
        # out_seg_dst = softmax(out_seg_dst).squeeze()  # * out_mask_dst.gt(0.5).float().unsqueeze(0)
        # out_seg_src = out_seg_src.argmax(dim=0)
        # scipy.io.savemat('seg.mat', dict(seg_src=seg_src.squeeze().cpu().numpy(), out_seg_src=out_seg_src.squeeze().cpu().numpy()))


def val():
    net_train.eval()
    running_loss = 0
    batch_loop = 0
    num_iter_per_epoch = len(dataset_val)
    iou_epoch = []
    for data, target in dataset_val:
        if batch_loop == num_iter - 1:
            break
        #     data: Dict[str, torch.Tensor]
        #     # reference
        #     rgb = data['image']  # Bx3xHxW

        depth_src = data['depth_src']  # Bx1xHxW
        # k_src = data['k_src']
        # input_1 = torch.cat((X, Y, depth_src), dim=1)
        seg_src = data['seg_src']

        # trans = data['trans']  # Bx4x4
        depth_dst = target['depth_dst']  # Bx1xHxW
        # k_dst = target['k_dst']  # Bx1xHxW
        seg_dst = target['seg_dst']

        depth_src = torch.from_numpy(depth_src).to(device)
        depth_dst = torch.from_numpy(depth_dst).to(device)
        seg_src = torch.from_numpy(seg_src).to(device)
        seg_dst = torch.from_numpy(seg_dst).to(device)

        mask_src_depth = depth_src.lt(100).float().squeeze()
        mask_dst_depth = depth_dst.lt(100).float().squeeze()

        # mask_flaw_src = mask_src_depth.gt(0).squeeze() * seg_src.eq(0)
        # seg_src[mask_flaw_src] = 1
        #
        # mask_flaw_dst = mask_dst_depth.gt(0).squeeze() * seg_dst.eq(0)
        # seg_dst[mask_flaw_dst] = 1

        mask_src = seg_src.gt(0).float()
        mask_dst = seg_dst.gt(0).float()


        depth_src = depth_src * mask_src
        depth_dst = depth_dst * mask_dst

        if is_merge_label:
            ## for 01_250

            seg_src[seg_src.gt(1)] = 2
            seg_dst[seg_dst.gt(1)] = 2


        seg_src = seg_src.unsqueeze(0).long()
        seg_dst = seg_dst.unsqueeze(0).long()

        IDs = seg_src.unique()

        if len(depth_src.size()) == 3:
            depth_src = depth_src.unsqueeze(0)
            depth_dst = depth_dst.unsqueeze(0)

        out_seg_src = net_train(depth_src)
        out_seg_dst = net_train(depth_dst)

        loss_seg_src = criterion(out_seg_src, seg_src).squeeze()
        loss_seg_dst = criterion(out_seg_dst, seg_dst).squeeze()
        seg_src = seg_src.squeeze()
        seg_dst = seg_dst.squeeze()
        loss = torch.Tensor([0]).to(device)

        for id in IDs:
            loss = loss + loss_seg_src[seg_src.eq(id)].mean() + loss_seg_dst[seg_dst.eq(id)].mean()
        f = plt.figure(figsize=(30,10))
        # ax1 = f.add_subplot(121)
        # ax2 = f.add_subplot(122)
        ax3 = f.add_subplot(121)
        ax4 = f.add_subplot(122)


        # #
        # # out_mask_src = softmax(out_mask_src).squeeze()[1]
        # # ax1.plt(out_mask_src)
        # # out_mask_dst = softmax(out_mask_dst).squeeze()[1]
        # # ax2.plt(out_mask_dst)
        # #
        out_seg_src = softmax(out_seg_src).squeeze()#
        out_seg_src = out_seg_src.cpu().detach().argmax(dim = 0).numpy()
        # out_seg_src= np.moveaxis(out_seg_src, 0, 2)  
        ax3.set_title('out seg src val, epoch: %i' %epoch_loop)
        ax3.imshow( out_seg_src)
        ax3.axis('off')
        out_seg_dst = softmax(out_seg_dst).squeeze()##
        out_seg_dst = out_seg_dst.cpu().detach().argmax(dim = 0).numpy()
        # out_seg_dst = np.moveaxis(out_seg_dst, 0, 2)
        ax4.set_title('out seg dst val epoch: %i' %epoch_loop) 
        ax4.imshow(out_seg_dst)
        ax4.axis('off')
        #
        running_loss = running_loss + loss.item()

        print('val_epoch {}: {}/{}: loss: {:.4f}'.format(
            epoch_loop + 1,
            batch_loop + 1,
            num_iter_per_epoch,
            running_loss / (batch_loop + 1)))
        batch_loop += 1
        if batch_loop == num_iter - 1:
            loss_val.append(running_loss/ (batch_loop + 1))



        # scipy.io.savemat('mask.mat', dict(mask_src=mask_src.squeeze().cpu().numpy(), out_mask_src=out_mask_src.squeeze().cpu().numpy()))
        # out_seg_src = softmax(out_seg_src).squeeze()  # * out_mask_src.gt(0.5).float().unsqueeze(0)
        # out_seg_dst = softmax(out_seg_dst).squeeze()  # * out_mask_dst.gt(0.5).float().unsqueeze(0)
        # out_seg_src = out_seg_src.argmax(dim=0)
        # out_seg_dst = out_seg_dst.argmax(dim=0)
        # scipy.io.savemat('seg.mat', dict(seg_src=seg_src.squeeze().cpu().numpy(), out_seg_src=out_seg_src.squeeze().cpu().numpy()))

    #     iou_epoch = 0
    #     iou = 0
    #     for id in IDs:
    #         if id == 0:
    #             continue
    #         iou += iou_pytorch(out_seg_src.eq(id).unsqueeze(0), seg_src.eq(id).unsqueeze(0)) + iou_pytorch(out_seg_dst.eq(id).unsqueeze(0), seg_dst.eq(id).unsqueeze(0))

    #     iou /= (len(IDs) - 1)
    #     iou /= 2
    #     if not iou_epoch:
    #         iou_epoch = iou
    #     else:
    #         iou_epoch = torch.cat(iou_epoch, iou)
    # iou_val.append(iou_epoch.mean().item)

"""# Training Loop"""

# import torch
  # torch.cuda.empty_cache()
  print(torch.__version__)

file_loss = os.path.join(net_dir, 'MS.npy')
file_iou = os.path.join(net_dir, 'IoU.npy')
loss_train = []
loss_val = []
iou_train = []
iou_val = []

for epoch_loop in range(args.epoch_max):#args.epoch_max)
    print("Train____________________________________________________")
    with torch.set_grad_enabled(True):
        train()
    # train()
    print("Val____________________________________________________")
    with torch.set_grad_enabled(False):
        val()


    net_name = os.path.join(net_dir, 'net-epoch-{}.pth'.format(epoch_loop + 1))
    torch.save(net_train.state_dict(), net_name)
    np.save(file_loss, [loss_train, loss_val ])
    np.save(file_iou, [iou_train, iou_val])

#     norms_dst = target['norms_dst']  # Bx3xHxW

    # scipy.io.savemat('dep.mat', dict(depth1=depth_src.squeeze(), depth2=depth_dst.squeeze()))
    #
    # depth_src[depth_src.eq(100)].nonzero()
    #
    #
    #
    # fig1 = plt.figure(figsize=(8, 8))
    # f1 = plt.imshow(depth_src.squeeze())
    # plt.axis('off')
    # plt.show()
    #
    # fig2 = plt.figure(figsize=(8, 8))
    # f2 = plt.imshow(depth_dst.squeeze())
    # plt.axis('off')
    # plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %debug

# # predictions = model.forward(my_images.unsqueeze(1).float().cuda())
# # net_train.state_dict()
# # Load
# model = UNET(is_xy=args.is_xy, num_class=3)
# model.load_state_dict(torch.load(net_name))
# model.eval()
# model.forward()

import matplotlib.pyplot as plt
    # np.save(file_loss, [loss_train, loss_val ])
    # np.save(file_iou, [iou_train, iou_val])
# plt.yscale('log')

# Unet_train_loss = loss_train
# plt.plot(Unet_train_loss)
# # Unet_val_loss = loss_val
# plt.plot(Unet_val_loss)
# Unet_train_shuffled_loss = loss_train
# plt.plot(Unet_train_shuffled_loss)
# Unet_val_shuffled_loss = loss_val
# plt.plot(Unet_val_shuffled_loss)
plt.plot(loss_train)
plt.plot(loss_val)
# plt.legend(labels)
# plt.ylim((0,5))
# plt.xlim((0,50))
# print(loss_val)
# fig, axes = plt.subplots(figsize=(10, 4))
# axes.plot(Unet_train_loss, label = 'Training')
# axes.plot(Unet_val_loss, label = 'Validation')
# axes.plot(Unet_train_shuffled_loss, label = 'Training shuffled')
# axes.plot(Unet_val_shuffled_loss, label = 'Validation shuffled')
# axes.plot(loss_train, label = 'Training reduced Unet')
# axes.plot(loss_val, label = 'Validation reduced Unet')
# np.save('/content/train_loss',Unet_train_loss)
# np.save('/content/Val_loss',Unet_val_loss)
# # axes.set_title('Training and Validation curves for VGG-16 Models')
# axes.set_ylabel('loss')
# axes.set_xlabel('Epoch')
# axes.set_ylim([0,5])
# axes.grid()
# axes.legend(loc = 'best',fontsize = 'medium')#,fontsize = 'large'

# !pip install Imath
# !apt-get install Imath

# import os
# import numpy as np 
# from matplotlib import pyplot as plt
# from matplotlib import cm
# from mpl_toolkits import mplot3d
# #from mpl_toolkits.mplot3d import Axes3D
# # import OpenEXR
# import array
# # import Imath ##Imath does not work in colab

# # FLOAT = Imath.PixelType(Imath.PixelType.FLOAT) ## Had to comment this as Imath could not be imported.

# # res_path = "/mnt/data/jsanchez/Datasets/ipalm_cloth"
# # res_path = "/Users/jsanchez/Datasets/ipalm_gown"
# # res_path = "/home/yurun/Research/mydata/ipalm_cloth3_normals"
# res_path =  "drive/MyDrive/Research/ipalm_cloth4"


# data_to_check = "occlusions"
# sim_to_check = 0
# frame_to_check = 250
# camera = 0

# # # test1 - to remove
# # file_temp = "%s/temp.npy" % res_path
# # cur_data = np.load(file_temp)

# # plt.imshow(cur_data, interpolation='nearest')
# # plt.show()

# # # test 2
# # path_data = "%s/%s/sim%04d" % (res_path, data_to_check, sim_to_check)

# # files_folder = [f for f in os.listdir(path_data) if f.endswith('.npy')]
# # files_folder.sort()

# # #ax = plt.axes(projection='3d')

# # for f_data in files_folder:

# #     # read data
# #     f_path = "%s/%s" % (path_data, f_data)
# #     cur_data = np.load(f_path)

# #     # xdata = cur_data[:,:,0]
# #     # ydata = cur_data[:,:,1]
# #     # zdata = cur_data[:,:,2]

# #     plt.imshow(cur_data, interpolation='nearest')
# #     #ax.scatter3D(xdata, ydata, zdata, cmap='Greens') # for vertices
# #     plt.show()

# # print(cur_data[27][64])



# # Test check segmentation files!
# # def count_colours(src):
# #     unique, counts = np.unique(src.reshape(-1, src.shape[-1]), axis=0, return_counts=True)
# #     return counts.size

# # f_path = "/Users/jsanchez/Downloads/test_seg.exr" 
# # cur_data = OpenEXR.InputFile(f_path)

# # # d_data = np.reshape([array.array('f', cur_data.channel(Chan, FLOAT)).tolist() for Chan in ("R", "G", "B")], (3, 512, 512))
# # d_data = np.transpose(np.reshape([array.array('f', cur_data.channel(Chan, FLOAT)).tolist() for Chan in ("R", "G", "B")], (3, 512, 512)), (1, 2, 0))
# # # If no background max value is Inf, so it can't display range values correctly, because it goes from
# # # x to Inf, which only displays 2 colors then
# # plt.imshow(d_data)
# # plt.show()







# # plot different data

# resx = 512
# resy = 512
# inresx = 128
# inresy = 128

# # data_to_check = "depth"
# # path_data = "%s/%s/sim%04d/Camera%02d" % (res_path, data_to_check, sim_to_check, camera)
# # f_data = "%04d.exr" % frame_to_check
# # f_path = "%s/%s" % (path_data, f_data)
# # cur_data = OpenEXR.InputFile(f_path)
# #
# # d_data = np.reshape([array.array('f', cur_data.channel(Chan, FLOAT)).tolist() for Chan in ("R")], (resx, resy))
# # # If no background max value is Inf, so it can't display range values correctly, because it goes from
# # # x to Inf, which only displays 2 colors then
# # plt.imshow(d_data, interpolation='nearest')
# # plt.show()


# # data_to_check = "occlusions"
# # path_data = "%s/%s/sim%04d/Camera%02d" % (res_path, data_to_check, sim_to_check, camera)
# # f_data = "%04d.npz" % frame_to_check
# # f_path = "%s/%s" % (path_data, f_data)
# # cur_data = np.load(f_path)
# # cur_data = cur_data.f.arr_0
# #
# # # get shape : for depth data conversion
# # resx = cur_data.shape[0]
# # resy = cur_data.shape[1]

# # plt.imshow(cur_data, interpolation='nearest')
# # plt.show()

# # data_to_check = "distances"
# # path_data = "%s/%s/sim%04d/Camera%02d" % (res_path, data_to_check, sim_to_check, camera)
# # f_data = "%04d.npz" % frame_to_check
# # f_path = "%s/%s" % (path_data, f_data)
# # cur_data = np.load(f_path)
# # cur_data = cur_data.f.arr_0
# #
# # plt.imshow(cur_data, interpolation='nearest')
# # plt.show()

# data_to_check = "uvpixels"
# path_data = "%s/%s/sim%04d/Camera%02d" % (res_path, data_to_check, sim_to_check, camera)
# f_data = "%04d.npz" % frame_to_check
# f_path = "%s/%s" % (path_data, f_data)
# cur_data = np.load(f_path)
# cur_data = cur_data.f.arr_0

# # plt.imshow(cur_data[:,:,0], interpolation='nearest')
# # plt.show()
# # plt.imshow(cur_data[:,:,1], interpolation='nearest')
# # plt.show()

# ext_data = np.concatenate((cur_data,)*2, axis=-1)
# print(ext_data.shape)
# plt.imshow(ext_data, interpolation='nearest')
# plt.show()

# proj_img = np.zeros((resx, resy), dtype=int)
# for y in range(inresy):
#     for x in range(inresx):
#         val = cur_data[x,y]
#         y_coord = int(511 - val[1] * resy)
#         x_coord = int(val[0] * resx)
#         proj_img[y_coord, x_coord] = 1
# plt.imshow(proj_img, interpolation='nearest')
# plt.show()


# data_to_check = "vertices"
# path_data = "%s/%s/sim%04d/Camera%02d" % (res_path, data_to_check, sim_to_check, camera)
# f_data = "%04d.npz" % frame_to_check
# f_path = "%s/%s" % (path_data, f_data)
# cur_data = np.load(f_path)
# cur_data = cur_data.f.arr_0

# # ax = plt.axes(projection='3d')
# # # xdata = cur_data[0::19,0::19,0]
# # # ydata = cur_data[0::19,0::19,1]
# # # zdata = cur_data[0::19,0::19,2]
# # xdata = cur_data[0::4,0::4,0]
# # ydata = cur_data[0::4,0::4,1]
# # zdata = cur_data[0::4,0::4,2]
# # ax.scatter3D(xdata, ydata, zdata, cmap='Greens') # for vertices
# plt.imshow(cur_data, interpolation='nearest')
# plt.show()


# # fig = plt.figure()
# # ax = fig.gca(projection='3d')

# data_to_check = "normals"
# path_data = "%s/%s/sim%04d/Camera%02d" % (res_path, data_to_check, sim_to_check, camera)
# f_data = "%04d.npz" % frame_to_check
# f_path = "%s/%s" % (path_data, f_data)
# cur_data = np.load(f_path)
# cur_data = cur_data.f.arr_0

# # ax = plt.axes(projection='3d')
# # # xnorm = cur_data[0::19,0::19,0]
# # # ynorm = cur_data[0::19,0::19,1]
# # # znorm = cur_data[0::19,0::19,2]
# # xnorm = cur_data[0::4,0::4,0]
# # ynorm = cur_data[0::4,0::4,1]
# # znorm = cur_data[0::4,0::4,2]
# # ax.quiver(xdata, ydata, zdata, xnorm, ynorm, znorm) # for vertices
# plt.imshow(cur_data, interpolation='nearest')
# plt.show()